# Ajuste de parametros de algoritmos


from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier


import numpy as np

import pickle
with open ('credit.pkl', 'rb') as f:
    X_credit_treinamento , y_credit_treinamento , X_credit_teste , y_credit_teste = pickle.load(f)


X_credit_treinamento.shape , y_credit_treinamento.shape
print (X_credit_treinamento.shape , y_credit_treinamento.shape)

print(X_credit_teste.shape , y_credit_teste.shape)

X_credit = np.concatenate((X_credit_treinamento, X_credit_teste ), axis= 0)
print(X_credit.shape)

print(X_credit)

y_credit = np.concatenate((y_credit_treinamento, y_credit_teste), axis=0)
print(y_credit.shape)

print(y_credit)

#arvore de decis√£o
DecisionTreeClassifier()
parametros = {'criterion': ['gini', 'entropy'],
              'splitter':['best', 'random'],
              'min_samples_split': [2,5,10],
              'min_samples_leaf': [1,5,10]}


grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=parametros)
grid_search.fit(X_credit, y_credit)

melhores_parametros = grid_search.best_params_
melhor_resultado = grid_search.best_score_
print(melhores_parametros)
print(melhor_resultado)

#random forest
RandomForestClassifier()
parametros = {'criterion': ['gini', 'entropy'],
              'n_estimators':[10,40,100,150],
              'min_samples_split': [2,5,10],
              'min_samples_leaf': [1,5,10]}

grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parametros)
grid_search.fit(X_credit, y_credit)

melhores_parametros = grid_search.best_params_
melhor_resultado = grid_search.best_score_
print(melhores_parametros)
print(melhor_resultado)


#KNN
KNeighborsClassifier()
parametros = {'n_neighbors': [3,5,10,20],
              'p': [1,2]}

grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=parametros)
grid_search.fit(X_credit, y_credit)

melhores_parametros = grid_search.best_params_
melhor_resultado = grid_search.best_score_
print(melhores_parametros)
print(melhor_resultado)

#logistica
LogisticRegression()
parametros =  {'tol': [0.0001,0.00001,0.000001],
               'C': [1.0,1.5,2.0],
               'solver':['lbfgs', 'sag','saga']}

grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=parametros)
grid_search.fit(X_credit, y_credit)

melhores_parametros = grid_search.best_params_
melhor_resultado = grid_search.best_score_
print(melhores_parametros)
print(melhor_resultado)


#svm
SVC()
parametros = {'tol': [0.001,0.0001,0.00001],
             'C' : [1.0,1.5,2.0],
              'kernel': ['rbf', 'linear', 'poly','sigmoid'] }

grid_search = GridSearchCV(estimator=SVC(), param_grid=parametros)
grid_search.fit(X_credit, y_credit)

melhores_parametros = grid_search.best_params_
melhor_resultado = grid_search.best_score_
print(melhores_parametros)
print(melhor_resultado)


#rede neural 
MLPClassifier()
parametros = {'activation': ['relu','logistic', 'tahn'],
              'solver': ['adam', 'sgd'],
              'batch_size': [10,56]}

grid_search = GridSearchCV(estimator=MLPClassifier(), param_grid=parametros)
grid_search.fit(X_credit, y_credit)

melhores_parametros = grid_search.best_params_
melhor_resultado = grid_search.best_score_
print(melhores_parametros)
print(melhor_resultado)







resultados_arvore = []
resultados_random_forest = []
resultados_knn = []
resultados_logistica = []
resultados_svm = []
resultados_rede_neural = []

for i in range(30):     
    #print(i)
    kfold = KFold(n_splits=10, shuffle= True, random_state=i)

    arvore = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=1, min_samples_split=5, splitter='best')
    scores = cross_val_score(arvore, X_credit, y_credit, cv = kfold)
    
    #print(scores)
    #print(scores.mean())
    resultados_arvore.append(scores.mean())
    print(resultados_arvore)

    random_forest = RandomForestClassifier(criterion = 'entropy',min_samples_lear = 1, min_samples_split=5, n_estimator=10)
    scores = cross_val_score(random_forest, X_credit, y_credit, cv = kfold)
    resultados_random_forest.append(scores.mean())


    print(resultados_arvore , resultados_random_forest)
